{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "environ['PYSPARK_PYTHON'] = \"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/venv/bin/python\"\n",
    "session = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"web-archive-query-log-num-results-per-serp\") \\\n",
    "    .config(\"spark.executor.instances\", 3) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = session.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "global_data_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/\")\n",
    "global_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = global_data_dir / \"focused\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# TODO: For final evaluation, use the full corpus.\n",
    "# corpus_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/focused/corpus\")\n",
    "# queries_dir = corpus_dir / \"queries-2023-02-XX\"\n",
    "# documents_dir = corpus_dir / \"documents-2023-02-XX\"\n",
    "corpus_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/focused/sample-corpus\")\n",
    "queries_dir = corpus_dir / \"queries-2023-02-18\"\n",
    "documents_dir = corpus_dir / \"documents-2023-02-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"figures\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_jsonl(service: str, base_type: str):\n",
    "    base_path = data_dir / base_type / service\n",
    "    if not base_path.exists():\n",
    "        return []\n",
    "    yield from base_path.glob(\"*/*.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_warc(service: str, base_type: str):\n",
    "    base_path = data_dir / base_type / service\n",
    "    if not base_path.exists():\n",
    "        return []\n",
    "    for path in base_path.glob(\"*/*\"):\n",
    "        if path.is_dir():\n",
    "            yield path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gzip import GzipFile\n",
    "\n",
    "\n",
    "def count_jsonl(path: Path) -> int:\n",
    "    print(f\"Count JSONL records in {path}.\")\n",
    "    try:\n",
    "        with GzipFile(path, \"r\") as file:\n",
    "            return sum(1 for _ in file)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads, JSONDecodeError\n",
    "from typing import Iterator\n",
    "\n",
    "def read_jsonl(path: Path) -> Iterator:\n",
    "    print(f\"Read JSONL records in {path}.\")\n",
    "    try:\n",
    "        with GzipFile(path, \"r\") as gzip_file:\n",
    "            for line in gzip_file:\n",
    "                try:\n",
    "                    url = loads(line)\n",
    "                except JSONDecodeError:\n",
    "                    continue\n",
    "                yield url\n",
    "    except:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"services.txt\").open(\"rt\") as file:\n",
    "    alexa_services = [\n",
    "        line.strip()\n",
    "        for line in file\n",
    "        if line\n",
    "    ]\n",
    "len(alexa_services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "num_search_results_per_serp = sc.parallelize(alexa_services, 1000)\\\n",
    "    .filter(lambda service: service not in top_services.keys())\\\n",
    "    .flatMap(lambda service: paths_jsonl(service, \"archived-parsed-serps\"))\\\n",
    "    .repartition(10_000)\\\n",
    "    .flatMap(read_jsonl)\\\n",
    "    .filter(lambda serp: serp[\"results\"] is not None)\\\n",
    "    .map(lambda serp: len(serp[\"results\"]))\\\n",
    "    .mean()\n",
    "num_search_results_per_serp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
