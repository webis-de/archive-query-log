{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf28ecc-8f7c-45ee-ad56-7086a738d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps, loads, JSONDecodeError\n",
    "from datetime import datetime\n",
    "from uuid import UUID, uuid5, NAMESPACE_URL\n",
    "from gzip import GzipFile\n",
    "from io import TextIOWrapper\n",
    "from csv import reader, writer\n",
    "from pathlib import Path\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from random import shuffle, sample\n",
    "from tqdm import tqdm\n",
    "from fastwarc import ArchiveIterator, FileStream, WarcRecord, WarcRecordType, GZipStream\n",
    "from fastwarc.stream_io import PythonIOStreamAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35756464-ec74-4453-826a-3395699f4d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.23.88.185:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>web-archive-query-log-corpus</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=web-archive-query-log-corpus>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.executor.instances\", 3)\n",
    "])\n",
    "sc = SparkContext(\n",
    "    master=\"yarn\",\n",
    "    appName=\"web-archive-query-log-corpus\",\n",
    "    conf=conf,\n",
    ")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7606424-96ee-4b14-934d-b006b43bde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28df0d23-001b-4699-9627-e4c7e20bd609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_data_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/\")\n",
    "global_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054aebc7-2ede-49ef-9d7b-aea5027edcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/focused')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = global_data_dir / \"focused\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f376fdf-792d-4df0-afc2-8302b39dd44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62299"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_paths = [\n",
    "    path.relative_to(data_dir / \"archived-urls\").with_name(path.name[:-len(\".jsonl.gz\")])\n",
    "    for path in data_dir.glob(\"archived-urls/*/*/*.jsonl.gz\")\n",
    "]\n",
    "len(relative_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe2cbe2-4449-48fa-a332-1bc343c4c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_jsonl(path: Path, base_type: str) -> dict:\n",
    "    jsonl_path = data_dir / base_type / path.with_suffix(\".jsonl.gz\")\n",
    "    if not jsonl_path.exists():\n",
    "        return {}\n",
    "    index_path = data_dir / base_type / path.with_suffix(\".index\")\n",
    "    offset = 0\n",
    "    index = {}\n",
    "    with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "        for line in tqdm(gzip_file, desc=\"Index JSONL\"):\n",
    "            try:\n",
    "                record = loads(line)\n",
    "            except JSONDecodeError:\n",
    "                print(f\"Could not index {line} at {path}.\")\n",
    "                return {} # TODO: Maybe just skip the one faulty record.\n",
    "            record_id = uuid5(\n",
    "                NAMESPACE_URL,\n",
    "                f\"{record['timestamp']}:{record['url']}\",\n",
    "            )\n",
    "            index[record_id] = (\n",
    "                jsonl_path,\n",
    "                offset,\n",
    "            )\n",
    "            offset = gzip_file.tell()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a190e1e7-536b-4045-add4-a0a3ac95406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_jsonl_snippets(path: Path, base_type: str) -> dict:\n",
    "    jsonl_path = data_dir / base_type / path.with_suffix(\".jsonl.gz\")\n",
    "    if not jsonl_path.exists():\n",
    "        return {}\n",
    "    index_path = data_dir / base_type / path.with_suffix(\".snippets.index\")\n",
    "    offset = 0\n",
    "    index = {}\n",
    "    with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "        for line in tqdm(gzip_file, desc=\"Index JSONL snippets\"):\n",
    "            try:\n",
    "                record = loads(line)\n",
    "            except JSONDecodeError:\n",
    "                print(f\"Could not index {line} at {path}.\")\n",
    "                return {} # TODO: Maybe just skip the one faulty record.\n",
    "            for snippet_index, snippet in enumerate(record[\"results\"]):\n",
    "                record_id = uuid5(\n",
    "                    NAMESPACE_URL,\n",
    "                    f\"{record['timestamp']}:{snippet['url']}\",\n",
    "                )\n",
    "                index[record_id] = (\n",
    "                    jsonl_path,\n",
    "                    offset,\n",
    "                    snippet_index,\n",
    "                )\n",
    "            offset = gzip_file.tell()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223fe573-e53c-43b4-a677-0ba43104df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_warc(path: Path, base_type: str) -> dict:\n",
    "    warc_path = data_dir / base_type / path\n",
    "    if not warc_path.exists():\n",
    "        return {}\n",
    "    index_path = data_dir / base_type / path.with_suffix(\".index\")\n",
    "    index = {}\n",
    "    for warc_child_path in warc_path.iterdir():\n",
    "        if warc_child_path.name.startswith(\".\"):\n",
    "            continue\n",
    "        try:\n",
    "            stream = FileStream(str(warc_child_path.absolute()))\n",
    "            records = ArchiveIterator(\n",
    "                stream,\n",
    "                record_types=WarcRecordType.response,\n",
    "                parse_http=False,\n",
    "            )\n",
    "            for record in tqdm(records, desc=\"Index WARC\"):\n",
    "                record: WarcRecord\n",
    "                offset = record.stream_pos\n",
    "                record_url_header = record.headers[\"Archived-URL\"]\n",
    "                try:\n",
    "                    record_url = loads(record_url_header)\n",
    "                except JSONDecodeError:\n",
    "                    print(f\"Could not index {record_url_header} at {path}.\")\n",
    "                    return {} # TODO: Maybe just skip the one faulty record.\n",
    "                record_id = uuid5(\n",
    "                    NAMESPACE_URL,\n",
    "                    f\"{record_url['timestamp']}:{record_url['url']}\",\n",
    "                )\n",
    "                index[record_id] = (\n",
    "                    warc_child_path,\n",
    "                    offset,\n",
    "                )\n",
    "        except:\n",
    "            raise Exception(f\"Could not read WARC file {warc_child_path}.\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3852e31-88e1-49de-891e-82976b1dec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_snippet(\n",
    "    archived_search_result_snippet_index: dict,\n",
    "    archived_raw_search_result_index: dict,\n",
    "    archived_parsed_search_result_index: dict,\n",
    "    archived_search_result_snippet: dict,\n",
    "    archived_search_result_snippet_id: UUID,\n",
    ") -> dict:\n",
    "    print(f\"Process archived search result snippet ID {archived_search_result_snippet_id}.\")\n",
    "    archived_search_result_snippet_location = archived_search_result_snippet_index.get(archived_search_result_snippet_id)\n",
    "    archived_raw_search_result_location = archived_raw_search_result_index.get(archived_search_result_snippet_id)\n",
    "    archived_parsed_search_result_location = archived_parsed_search_result_index.get(archived_search_result_snippet_id)\n",
    "    \n",
    "    if archived_search_result_snippet_location is not None:\n",
    "        with GzipFile(archived_search_result_snippet_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_search_result_snippet_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_search_result_snippet = loads(line)\n",
    "    else:\n",
    "        print(f\"Could not find archived search result snippet ID {archived_search_result_snippet_id}.\")\n",
    "        return None\n",
    "    \n",
    "    wayback_timestamp = datetime.fromtimestamp(archived_search_result_snippet[\"timestamp\"]).strftime(\"%Y%m%d%H%M%S\")\n",
    "    wayback_url = f\"https://web.archive.org/web/{wayback_timestamp}/{archived_search_result_snippet['url']}\"\n",
    "    wayback_raw_url = f\"https://web.archive.org/web/{wayback_timestamp}id_/{archived_search_result_snippet['url']}\"\n",
    "    \n",
    "    document = {\n",
    "        \"id\": str(archived_search_result_snippet_id),\n",
    "        \"url\": archived_search_result_snippet[\"url\"],\n",
    "        \"timestamp\": archived_search_result_snippet[\"timestamp\"],\n",
    "        \"wayback_url\": wayback_url,\n",
    "        \"wayback_raw_url\": wayback_raw_url,\n",
    "        \"snippet_rank\": archived_search_result_snippet[\"rank\"],\n",
    "        \"snippet_title\": archived_search_result_snippet[\"title\"],\n",
    "        \"snippet_text\": archived_search_result_snippet[\"snippet\"],\n",
    "        \"archived_snippet_location\": {\n",
    "            \"relative_path\": str(archived_search_result_snippet_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_search_result_snippet_location[1],\n",
    "            \"index\": archived_search_result_snippet_location[2],\n",
    "        } if archived_search_result_snippet_location is not None else None,\n",
    "        \"archived_raw_search_result_location\": {\n",
    "            \"relative_path\": str(archived_raw_search_result_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_raw_search_result_location[1],\n",
    "        } if archived_raw_search_result_location is not None else None,\n",
    "        \"archived_parsed_search_result_location\": {\n",
    "            \"relative_path\": str(archived_parsed_search_result_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_parsed_search_result_location[1],\n",
    "        } if archived_parsed_search_result_location is not None else None,\n",
    "    }\n",
    "    print(f\"Finished processing archived search result snippet ID {archived_search_result_snippet_id}.\")\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b50fe82-53a8-4f1f-abe2-5810e0e51476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(\n",
    "    service: str,\n",
    "    archived_urls_index: dict,\n",
    "    archived_query_urls_index: dict,\n",
    "    archived_raw_serps_index: dict,\n",
    "    archived_parsed_serps_index: dict,\n",
    "    archived_search_result_snippet_index: dict,\n",
    "    archived_raw_search_result_index: dict,\n",
    "    archived_parsed_search_result_index: dict,\n",
    "    archived_url_id: UUID,\n",
    ") -> tuple:\n",
    "    print(f\"Process archived URL ID {archived_url_id}.\")\n",
    "    \n",
    "    archived_url_location = archived_urls_index.get(archived_url_id)\n",
    "    archived_query_url_location = archived_query_urls_index.get(archived_url_id)\n",
    "    archived_raw_serp_location = archived_raw_serps_index.get(archived_url_id)\n",
    "    archived_parsed_serp_location = archived_parsed_serps_index.get(archived_url_id)\n",
    "    \n",
    "    if archived_url_location is not None:\n",
    "        with GzipFile(archived_url_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_url_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_url = loads(line)\n",
    "    else:\n",
    "        print(f\"Could not find archived URL ID {archived_url_id}.\")\n",
    "        return None\n",
    "    if archived_query_url_location is not None:\n",
    "        with GzipFile(archived_query_url_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_query_url_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_query_url = loads(line)\n",
    "    else:\n",
    "        archived_query_url = None\n",
    "    if archived_parsed_serp_location is not None:\n",
    "        with GzipFile(archived_parsed_serp_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_parsed_serp_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_parsed_serp = loads(line)\n",
    "    else:\n",
    "        archived_parsed_serp = None\n",
    "            \n",
    "    wayback_timestamp = datetime.fromtimestamp(archived_url[\"timestamp\"]).strftime(\"%Y%m%d%H%M%S\")\n",
    "    wayback_url = f\"https://web.archive.org/web/{wayback_timestamp}/{archived_url['url']}\"\n",
    "    wayback_raw_url = f\"https://web.archive.org/web/{wayback_timestamp}id_/{archived_url['url']}\"\n",
    "    \n",
    "    partial_documents = [\n",
    "        process_snippet(\n",
    "            archived_search_result_snippet_index,\n",
    "            archived_raw_search_result_index,\n",
    "            archived_parsed_search_result_index,\n",
    "            archived_search_result_snippet,\n",
    "        )\n",
    "        for archived_search_result_snippet in archived_parsed_serp[\"results\"]\n",
    "    ] if archived_parsed_serp is not None else None\n",
    "    \n",
    "    partial_query = {\n",
    "        \"id\": str(archived_url_id),\n",
    "        \"url\": archived_url[\"url\"],\n",
    "        \"timestamp\": archived_url[\"timestamp\"],\n",
    "        \"wayback_url\": wayback_url,\n",
    "        \"wayback_raw_url\": wayback_raw_url,\n",
    "        \"url_query\": archived_query_url[\"query\"] if archived_query_url is not None else None,\n",
    "        \"url_page\": archived_query_url[\"page\"] if archived_query_url is not None else None,\n",
    "        \"url_offset\": archived_query_url[\"offset\"] if archived_query_url is not None else None,\n",
    "        \"serp_query\": archived_parsed_serp[\"interpreted_query\"] if archived_parsed_serp is not None else None,\n",
    "        \"archived_url_location\":{\n",
    "            \"relative_path\": str(archived_url_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_url_location[1],\n",
    "        },\n",
    "        \"archived_query_url_location\":{\n",
    "            \"relative_path\": str(archived_query_url_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_query_url_location[1],\n",
    "        } if archived_query_url_location is not None else None,\n",
    "        \"archived_raw_serp_location\":{\n",
    "            \"relative_path\": str(archived_raw_serp_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_raw_serp_location[1],\n",
    "        } if archived_raw_serp_location is not None else None,\n",
    "        \"archived_parsed_serp_location\":{\n",
    "            \"relative_path\": str(archived_parsed_serp_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_parsed_serp_location[1],\n",
    "        } if archived_parsed_serp_location is not None else None,\n",
    "    }\n",
    "    \n",
    "    query = {\n",
    "        **partial_query,\n",
    "        \"service\": service,\n",
    "        \"results\": partial_documents,\n",
    "    }\n",
    "    documents = [\n",
    "        {\n",
    "            **partial_document,\n",
    "            \"service\": service,\n",
    "            \"query\": partial_query,\n",
    "        }\n",
    "        for partial_document in partial_documents\n",
    "    ] if partial_documents is not None else []\n",
    "    print(f\"Finished processing archived URL ID {archived_url_id}.\")\n",
    "    return query, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7385990-1ab8-40ad-8bcd-0e2a20ce6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relative_path_archived_ids(path: Path):\n",
    "    print(f\"Process relative path indices {path}.\")\n",
    "    service = path.parts[0]\n",
    "    archived_urls_index = index_jsonl(path, \"archived-urls\")\n",
    "    archived_query_urls_index = index_jsonl(path, \"archived-query-urls\")\n",
    "    archived_raw_serps_index = index_warc(path, \"archived-raw-serps\")\n",
    "    archived_parsed_serps_index = index_jsonl(path, \"archived-parsed-serps\")\n",
    "    archived_search_result_snippets_index = index_jsonl_snippets(path, \"archived-parsed-serps\")\n",
    "    archived_raw_search_results_index = index_warc(path, \"archived-raw-search-results\")\n",
    "    archived_parsed_search_results_index = index_jsonl(path, \"archived-parsed-search-results\")\n",
    "    archived_ids = archived_urls_index.keys()\n",
    "    if SAMPLE_CORPUS:\n",
    "        archived_ids = sample(archived_ids, min(len(archived_ids), 10))\n",
    "    for archived_id in tqdm(archived_ids, desc=\"Yield archived IDs.\"):\n",
    "        yield (\n",
    "            service, \n",
    "            archived_urls_index,\n",
    "            archived_query_urls_index,\n",
    "            archived_raw_serps_index,\n",
    "            archived_parsed_serps_index,\n",
    "            archived_search_result_snippets_index,\n",
    "            archived_raw_search_results_index,\n",
    "            archived_parsed_search_results_index,\n",
    "            archived_id,\n",
    "        )\n",
    "    print(f\"Finished processing relative path indices {path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e00951c-f8f9-4b5a-a9ac-6bd11b68f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_archived_id(task: tuple) -> tuple:\n",
    "    service, archived_urls_index, archived_query_urls_index, archived_raw_serps_index, \\\n",
    "        archived_parsed_serps_index, archived_search_result_snippets_index, \\\n",
    "        archived_raw_search_results_index, archived_parsed_search_results_index, archived_id = task\n",
    "    print(f\"Process archived ID {archived_id}.\")\n",
    "    query_documents = process_url(\n",
    "        service, \n",
    "        archived_urls_index,\n",
    "        archived_query_urls_index,\n",
    "        archived_raw_serps_index,\n",
    "        archived_parsed_serps_index,\n",
    "        archived_search_result_snippets_index,\n",
    "        archived_raw_search_results_index,\n",
    "        archived_parsed_search_results_index,\n",
    "        archived_id,\n",
    "    )\n",
    "    print(f\"Finished processing archived ID {archived_id}.\")\n",
    "    return query_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9948d16e-24d5-4d36-b14c-b89179f584ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_archived_id_query(query_documents: tuple):\n",
    "    query, _ = query_documents\n",
    "    query_id = query['id']\n",
    "    print(f\"Process archived ID query {query_id}.\")\n",
    "    query = dumps(query)\n",
    "    print(f\"Finished processing archived ID query {query_id}.\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8fbefa6-551e-438b-a88f-c0529ac00bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_archived_id_documents(query_documents: tuple):\n",
    "    query, documents = query_documents\n",
    "    query_id = query['id']\n",
    "    print(f\"Process archived ID documents {query_id}.\")\n",
    "    for document in documents:\n",
    "        document = dumps(document)\n",
    "        yield document\n",
    "    print(f\"Finished processing archived ID documents {query_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe41453-43fc-4e48-a297-942a6ff044f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted web-archive-query-log/queries\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r web-archive-query-log/queries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb402be-8b5d-4706-80eb-823907f6e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(relative_paths)\n",
    "sc.parallelize(relative_paths)\\\n",
    "    .repartition(100_000)\\\n",
    "    .flatMap(process_relative_path_archived_ids)\\\n",
    "    .repartition(1000)\\\n",
    "    .map(process_archived_id)\\\n",
    "    .map(process_archived_id_query)\\\n",
    "    .repartition(100)\\\n",
    "    .saveAsTextFile(\n",
    "        \"web-archive-query-log/queries/\", \n",
    "        compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736295-25b3-48d4-a69f-197b6837ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r web-archive-query-log/documents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f81b6d-03af-4825-b9bf-764febf7725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(relative_paths)\n",
    "rdd = sc.parallelize(relative_paths)\\\n",
    "    .repartition(100_000)\\\n",
    "    .flatMap(process_relative_path_archived_ids)\\\n",
    "    .repartition(1000)\\\n",
    "    .map(process_archived_id)\\\n",
    "    .flatMap(process_archived_id_documents)\\\n",
    "    .repartition(100)\\\n",
    "    .saveAsTextFile(\n",
    "        \"web-archive-query-log/documents/\", \n",
    "        compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dbce4-e3a1-4906-aebe-c95e9930bca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
