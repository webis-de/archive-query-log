{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cf28ecc-8f7c-45ee-ad56-7086a738d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "environ['PYSPARK_PYTHON'] = \"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/venv/bin/python\"\n",
    "session = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"web-archive-query-log-corpus\") \\\n",
    "    .config(\"spark.executor.instances\", 5) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35756464-ec74-4453-826a-3395699f4d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.23.88.185:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>web-archive-query-log-corpus</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=web-archive-query-log-corpus>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = session.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7606424-96ee-4b14-934d-b006b43bde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_CORPUS = False\n",
    "# SAMPLE_CORPUS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28df0d23-001b-4699-9627-e4c7e20bd609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "global_data_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/\")\n",
    "global_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "054aebc7-2ede-49ef-9d7b-aea5027edcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/focused')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = global_data_dir / \"focused\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f376fdf-792d-4df0-afc2-8302b39dd44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62303"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_paths = [\n",
    "    path.relative_to(data_dir / \"archived-urls\").with_name(path.name[:-len(\".jsonl.gz\")])\n",
    "    for path in data_dir.glob(\"archived-urls/*/*/*.jsonl.gz\")\n",
    "]\n",
    "len(relative_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe2cbe2-4449-48fa-a332-1bc343c4c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid5\n",
    "from uuid import NAMESPACE_URL\n",
    "from tqdm.auto import tqdm\n",
    "from json import loads\n",
    "from gzip import GzipFile\n",
    "\n",
    "\n",
    "def index_jsonl(path: Path, base_type: str) -> dict:\n",
    "    jsonl_path = data_dir / base_type / path.with_suffix(\".jsonl.gz\")\n",
    "    if not jsonl_path.exists():\n",
    "        return {}\n",
    "    offset = 0\n",
    "    index = {}\n",
    "    try:\n",
    "        with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "            for line in tqdm(gzip_file, desc=\"Index JSONL\"):\n",
    "                try:\n",
    "                    record = loads(line)\n",
    "                except:\n",
    "                    print(f\"Could not index {line} at {path}.\")\n",
    "                    return {} # TODO: Maybe just skip the one faulty record.\n",
    "                record_id = uuid5(\n",
    "                    NAMESPACE_URL,\n",
    "                    f\"{record['timestamp']}:{record['url']}\",\n",
    "                )\n",
    "                index[record_id] = (\n",
    "                    jsonl_path,\n",
    "                    offset,\n",
    "                )\n",
    "                offset = gzip_file.tell()\n",
    "        return index\n",
    "    except:\n",
    "        print(f\"Could not read JSONL file at {path}.\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a190e1e7-536b-4045-add4-a0a3ac95406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_jsonl_snippets(path: Path, base_type: str) -> dict:\n",
    "    jsonl_path = data_dir / base_type / path.with_suffix(\".jsonl.gz\")\n",
    "    if not jsonl_path.exists():\n",
    "        return {}\n",
    "    offset = 0\n",
    "    index = {}\n",
    "    try:\n",
    "        with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "            for line in tqdm(gzip_file, desc=\"Index JSONL snippets\"):\n",
    "                try:\n",
    "                    record = loads(line)\n",
    "                except JSONDecodeError:\n",
    "                    print(f\"Could not index {line} at {path}.\")\n",
    "                    return {} # TODO: Maybe just skip the one faulty record.\n",
    "                for snippet_index, snippet in enumerate(record[\"results\"]):\n",
    "                    record_id = uuid5(\n",
    "                        NAMESPACE_URL,\n",
    "                        f\"{snippet['rank']}:{snippet['timestamp']}:{snippet['url']}\",\n",
    "                    )\n",
    "                    index[record_id] = (\n",
    "                        jsonl_path,\n",
    "                        offset,\n",
    "                        snippet_index,\n",
    "                    )\n",
    "                offset = gzip_file.tell()\n",
    "        return index\n",
    "    except:\n",
    "        print(f\"Could not read JSONL file at {path}.\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "223fe573-e53c-43b4-a677-0ba43104df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastwarc import FileStream, WarcRecordType, WarcRecord, ArchiveIterator\n",
    "\n",
    "\n",
    "def index_warc(path: Path, base_type: str) -> dict:\n",
    "    warc_path = data_dir / base_type / path\n",
    "    if not warc_path.exists():\n",
    "        return {}\n",
    "    index = {}\n",
    "    for warc_child_path in warc_path.iterdir():\n",
    "        if warc_child_path.name.startswith(\".\"):\n",
    "            continue\n",
    "        try:\n",
    "            stream = FileStream(str(warc_child_path.absolute()))\n",
    "            records = ArchiveIterator(\n",
    "                stream,\n",
    "                record_types=WarcRecordType.response,\n",
    "                parse_http=False,\n",
    "            )\n",
    "            for record in tqdm(records, desc=\"Index WARC\"):\n",
    "                record: WarcRecord\n",
    "                offset = record.stream_pos\n",
    "                record_url_header = record.headers[\"Archived-URL\"]\n",
    "                try:\n",
    "                    record_url = loads(record_url_header)\n",
    "                except JSONDecodeError:\n",
    "                    print(f\"Could not index {record_url_header} at {path}.\")\n",
    "                    return {} # TODO: Maybe just skip the one faulty record.\n",
    "                record_id = uuid5(\n",
    "                    NAMESPACE_URL,\n",
    "                    f\"{record_url['timestamp']}:{record_url['url']}\",\n",
    "                )\n",
    "                index[record_id] = (\n",
    "                    warc_child_path,\n",
    "                    offset,\n",
    "                )\n",
    "        except:\n",
    "            print(f\"Could not read WARC file at {warc_child_path}.\")\n",
    "            return {} # TODO: Maybe just skip the one faulty file.\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87894a99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> Optional[str]:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    from cld3 import get_language\n",
    "    language_prediction = get_language(text)\n",
    "    if language_prediction is None:\n",
    "        return None\n",
    "    return language_prediction.language.split(\"-\")[0] if language_prediction.is_reliable else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3852e31-88e1-49de-891e-82976b1dec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "from io import TextIOWrapper\n",
    "from uuid import UUID\n",
    "from bleach import clean\n",
    "\n",
    "def process_snippet(\n",
    "    service: str,\n",
    "    archived_search_result_snippet_index: dict,\n",
    "    archived_raw_search_result_index: dict,\n",
    "    archived_parsed_search_result_index: dict,\n",
    "    archived_search_result_snippet_id: UUID,\n",
    ") -> Optional[dict]:\n",
    "    print(f\"Process archived search result snippet ID {archived_search_result_snippet_id}.\")\n",
    "    archived_search_result_snippet_location = archived_search_result_snippet_index.get(archived_search_result_snippet_id)\n",
    "    archived_raw_search_result_location = archived_raw_search_result_index.get(archived_search_result_snippet_id)\n",
    "    archived_parsed_search_result_location = archived_parsed_search_result_index.get(archived_search_result_snippet_id)\n",
    "    \n",
    "    if archived_search_result_snippet_location is not None:\n",
    "        with GzipFile(archived_search_result_snippet_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_search_result_snippet_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_search_result_snippet = loads(line)[\"results\"][archived_search_result_snippet_location[2]]\n",
    "    else:\n",
    "        print(f\"Could not find archived search result snippet ID {archived_search_result_snippet_id}.\")\n",
    "        return None\n",
    "    print(archived_search_result_snippet)\n",
    "    \n",
    "    wayback_timestamp = datetime.fromtimestamp(archived_search_result_snippet[\"timestamp\"]).strftime(\"%Y%m%d%H%M%S\")\n",
    "    wayback_url = f\"https://web.archive.org/web/{wayback_timestamp}/{archived_search_result_snippet['url']}\"\n",
    "    wayback_raw_url = f\"https://web.archive.org/web/{wayback_timestamp}id_/{archived_search_result_snippet['url']}\"\n",
    "    title_and_text = archived_search_result_snippet[\"title\"]\n",
    "    if archived_search_result_snippet[\"snippet\"] is not None:\n",
    "        title_and_text += f\" {archived_search_result_snippet['snippet']}\"\n",
    "    title_and_text = clean(\n",
    "        title_and_text,\n",
    "        tags=[],\n",
    "        attributes=[],\n",
    "        protocols=[],\n",
    "        strip=True,\n",
    "        strip_comments=True,\n",
    "    )\n",
    "    language = detect_language(title_and_text)\n",
    "    \n",
    "    document = {\n",
    "        \"id\": str(archived_search_result_snippet_id),\n",
    "        \"url\": archived_search_result_snippet[\"url\"],\n",
    "        \"timestamp\": archived_search_result_snippet[\"timestamp\"],\n",
    "        \"wayback_url\": wayback_url,\n",
    "        \"wayback_raw_url\": wayback_raw_url,\n",
    "        \"snippet_rank\": archived_search_result_snippet[\"rank\"],\n",
    "        \"snippet_title\": archived_search_result_snippet[\"title\"],\n",
    "        \"snippet_text\": archived_search_result_snippet[\"snippet\"],\n",
    "        \"language\": language,\n",
    "        \"service\": service,\n",
    "        \"archived_snippet_location\": {\n",
    "            \"relative_path\": str(archived_search_result_snippet_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_search_result_snippet_location[1],\n",
    "            \"index\": archived_search_result_snippet_location[2],\n",
    "        } if archived_search_result_snippet_location is not None else None,\n",
    "        \"archived_raw_search_result_location\": {\n",
    "            \"relative_path\": str(archived_raw_search_result_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_raw_search_result_location[1],\n",
    "        } if archived_raw_search_result_location is not None else None,\n",
    "        \"archived_parsed_search_result_location\": {\n",
    "            \"relative_path\": str(archived_parsed_search_result_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_parsed_search_result_location[1],\n",
    "        } if archived_parsed_search_result_location is not None else None,\n",
    "    }\n",
    "    print(f\"Finished processing archived search result snippet ID {archived_search_result_snippet_id}.\")\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b50fe82-53a8-4f1f-abe2-5810e0e51476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(\n",
    "    service: str,\n",
    "    archived_urls_index: dict,\n",
    "    archived_query_urls_index: dict,\n",
    "    archived_raw_serps_index: dict,\n",
    "    archived_parsed_serps_index: dict,\n",
    "    archived_search_result_snippet_index: dict,\n",
    "    archived_raw_search_result_index: dict,\n",
    "    archived_parsed_search_result_index: dict,\n",
    "    archived_url_id: UUID,\n",
    ") -> Optional[tuple]:\n",
    "    print(f\"Process archived URL ID {archived_url_id}.\")\n",
    "    \n",
    "    archived_url_location = archived_urls_index.get(archived_url_id)\n",
    "    archived_query_url_location = archived_query_urls_index.get(archived_url_id)\n",
    "    archived_raw_serp_location = archived_raw_serps_index.get(archived_url_id)\n",
    "    archived_parsed_serp_location = archived_parsed_serps_index.get(archived_url_id)\n",
    "    \n",
    "    if archived_url_location is not None:\n",
    "        with GzipFile(archived_url_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_url_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_url = loads(line)\n",
    "    else:\n",
    "        print(f\"Could not find archived URL ID {archived_url_id}.\")\n",
    "        return None\n",
    "    if archived_query_url_location is not None:\n",
    "        with GzipFile(archived_query_url_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_query_url_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_query_url = loads(line)\n",
    "    else:\n",
    "        archived_query_url = None\n",
    "    if archived_parsed_serp_location is not None:\n",
    "        with GzipFile(archived_parsed_serp_location[0], \"rb\") as gzip_file:\n",
    "            gzip_file.seek(archived_parsed_serp_location[1])\n",
    "            with TextIOWrapper(gzip_file) as text_file:\n",
    "                line = text_file.readline()\n",
    "                archived_parsed_serp = loads(line)\n",
    "    else:\n",
    "        archived_parsed_serp = None\n",
    "            \n",
    "    wayback_timestamp = datetime.fromtimestamp(archived_url[\"timestamp\"]).strftime(\"%Y%m%d%H%M%S\")\n",
    "    wayback_url = f\"https://web.archive.org/web/{wayback_timestamp}/{archived_url['url']}\"\n",
    "    wayback_raw_url = f\"https://web.archive.org/web/{wayback_timestamp}id_/{archived_url['url']}\"\n",
    "    language = detect_language(archived_query_url[\"query\"]) \\\n",
    "        if archived_query_url is not None else None\n",
    "    \n",
    "    partial_documents = [\n",
    "        process_snippet(\n",
    "            service,\n",
    "            archived_search_result_snippet_index,\n",
    "            archived_raw_search_result_index,\n",
    "            archived_parsed_search_result_index,\n",
    "            uuid5(\n",
    "                NAMESPACE_URL,\n",
    "                f\"{snippet['rank']}:{snippet['timestamp']}:{snippet['url']}\",\n",
    "            ),\n",
    "        )\n",
    "        for snippet in archived_parsed_serp[\"results\"]\n",
    "    ] if archived_parsed_serp is not None else None\n",
    "    \n",
    "    partial_query = {\n",
    "        \"id\": str(archived_url_id),\n",
    "        \"url\": archived_url[\"url\"],\n",
    "        \"timestamp\": archived_url[\"timestamp\"],\n",
    "        \"wayback_url\": wayback_url,\n",
    "        \"wayback_raw_url\": wayback_raw_url,\n",
    "        \"url_query\": archived_query_url[\"query\"] if archived_query_url is not None else None,\n",
    "        \"url_page\": archived_query_url[\"page\"] if archived_query_url is not None else None,\n",
    "        \"url_offset\": archived_query_url[\"offset\"] if archived_query_url is not None else None,\n",
    "        \"serp_query\": archived_parsed_serp[\"interpreted_query\"] if archived_parsed_serp is not None else None,\n",
    "        \"language\": language,\n",
    "        \"service\": service,\n",
    "        \"archived_url_location\":{\n",
    "            \"relative_path\": str(archived_url_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_url_location[1],\n",
    "        },\n",
    "        \"archived_query_url_location\":{\n",
    "            \"relative_path\": str(archived_query_url_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_query_url_location[1],\n",
    "        } if archived_query_url_location is not None else None,\n",
    "        \"archived_raw_serp_location\":{\n",
    "            \"relative_path\": str(archived_raw_serp_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_raw_serp_location[1],\n",
    "        } if archived_raw_serp_location is not None else None,\n",
    "        \"archived_parsed_serp_location\":{\n",
    "            \"relative_path\": str(archived_parsed_serp_location[0].relative_to(global_data_dir)),\n",
    "            \"byte_offset\": archived_parsed_serp_location[1],\n",
    "        } if archived_parsed_serp_location is not None else None,\n",
    "    }\n",
    "    \n",
    "    query = {\n",
    "        **partial_query,\n",
    "        \"results\": partial_documents,\n",
    "    }\n",
    "    documents = [\n",
    "        {\n",
    "            **partial_document,\n",
    "            \"query\": partial_query,\n",
    "        }\n",
    "        for partial_document in partial_documents\n",
    "    ] if partial_documents is not None else []\n",
    "    print(f\"Finished processing archived URL ID {archived_url_id}.\")\n",
    "    return query, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7385990-1ab8-40ad-8bcd-0e2a20ce6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "def process_relative_path_archived_ids(path: Path):\n",
    "    print(f\"Process relative path indices {path}.\")\n",
    "    service = path.parts[0]\n",
    "    archived_urls_index = index_jsonl(path, \"archived-urls\")\n",
    "    archived_query_urls_index = index_jsonl(path, \"archived-query-urls\")\n",
    "    archived_raw_serps_index = index_warc(path, \"archived-raw-serps\")\n",
    "    archived_parsed_serps_index = index_jsonl(path, \"archived-parsed-serps\")\n",
    "    archived_search_result_snippets_index = index_jsonl_snippets(path, \"archived-parsed-serps\")\n",
    "    archived_raw_search_results_index = index_warc(path, \"archived-raw-search-results\")\n",
    "    archived_parsed_search_results_index = index_jsonl(path, \"archived-parsed-search-results\")\n",
    "    archived_ids = archived_urls_index.keys()\n",
    "    if SAMPLE_CORPUS:\n",
    "        archived_ids = sample(archived_ids, min(len(archived_ids), 10))\n",
    "    for archived_id in tqdm(archived_ids, desc=\"Yield archived IDs.\"):\n",
    "        yield (\n",
    "            service, \n",
    "            archived_urls_index,\n",
    "            archived_query_urls_index,\n",
    "            archived_raw_serps_index,\n",
    "            archived_parsed_serps_index,\n",
    "            archived_search_result_snippets_index,\n",
    "            archived_raw_search_results_index,\n",
    "            archived_parsed_search_results_index,\n",
    "            archived_id,\n",
    "        )\n",
    "    print(f\"Finished processing relative path indices {path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e00951c-f8f9-4b5a-a9ac-6bd11b68f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_archived_id(task: tuple) -> tuple:\n",
    "    service, archived_urls_index, archived_query_urls_index, archived_raw_serps_index, \\\n",
    "        archived_parsed_serps_index, archived_search_result_snippets_index, \\\n",
    "        archived_raw_search_results_index, archived_parsed_search_results_index, archived_id = task\n",
    "    print(f\"Process archived ID {archived_id}.\")\n",
    "    query_documents = process_url(\n",
    "        service, \n",
    "        archived_urls_index,\n",
    "        archived_query_urls_index,\n",
    "        archived_raw_serps_index,\n",
    "        archived_parsed_serps_index,\n",
    "        archived_search_result_snippets_index,\n",
    "        archived_raw_search_results_index,\n",
    "        archived_parsed_search_results_index,\n",
    "        archived_id,\n",
    "    )\n",
    "    print(f\"Finished processing archived ID {archived_id}.\")\n",
    "    return query_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9948d16e-24d5-4d36-b14c-b89179f584ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "\n",
    "\n",
    "def process_archived_id_query(query_documents: tuple):\n",
    "    query, _ = query_documents\n",
    "    query_id = query['id']\n",
    "    print(f\"Process archived ID query {query_id}.\")\n",
    "    query = dumps(query)\n",
    "    print(f\"Finished processing archived ID query {query_id}.\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8fbefa6-551e-438b-a88f-c0529ac00bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_archived_id_documents(query_documents: tuple):\n",
    "    query, documents = query_documents\n",
    "    query_id = query['id']\n",
    "    print(f\"Process archived ID documents {query_id}.\")\n",
    "    for document in documents:\n",
    "        document = dumps(document)\n",
    "        yield document\n",
    "    print(f\"Finished processing archived ID documents {query_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbe41453-43fc-4e48-a297-942a6ff044f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `web-archive-query-log/queries/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r web-archive-query-log/queries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c896c371-3f3e-4cce-a808-ac0443af00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(relative_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb402be-8b5d-4706-80eb-823907f6e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(relative_paths)\\\n",
    "    .repartition(100_000)\\\n",
    "    .flatMap(process_relative_path_archived_ids)\\\n",
    "    .repartition(200_000)\\\n",
    "    .map(process_archived_id)\\\n",
    "    .map(process_archived_id_query)\\\n",
    "    .repartition(100)\\\n",
    "    .saveAsTextFile(\n",
    "        \"web-archive-query-log/queries/\", \n",
    "        compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736295-25b3-48d4-a69f-197b6837ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r web-archive-query-log/documents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f81b6d-03af-4825-b9bf-764febf7725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(relative_paths)\\\n",
    "    .repartition(100_000)\\\n",
    "    .flatMap(process_relative_path_archived_ids)\\\n",
    "    .repartition(200_000)\\\n",
    "    .map(process_archived_id)\\\n",
    "    .flatMap(process_archived_id_documents)\\\n",
    "    .repartition(100)\\\n",
    "    .saveAsTextFile(\n",
    "        \"web-archive-query-log/documents/\", \n",
    "        compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c50de-e985-428b-b556-fa690e0067ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce09c2-7698-417d-be56-86649ced717d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
