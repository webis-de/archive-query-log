{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2cf28ecc-8f7c-45ee-ad56-7086a738d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "environ['PYSPARK_PYTHON'] = \"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/venv/bin/python\"\n",
    "session = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"web-archive-query-log-corpus\") \\\n",
    "    .config(\"spark.executor.instances\", 5) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35756464-ec74-4453-826a-3395699f4d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.23.233.210:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>web-archive-query-log-corpus</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=web-archive-query-log-corpus>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = session.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7606424-96ee-4b14-934d-b006b43bde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_CORPUS = False\n",
    "SAMPLE_CORPUS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28df0d23-001b-4699-9627-e4c7e20bd609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "global_data_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/\")\n",
    "global_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "054aebc7-2ede-49ef-9d7b-aea5027edcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/focused')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = global_data_dir / \"focused\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f376fdf-792d-4df0-afc2-8302b39dd44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61819"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "relative_paths = [\n",
    "    path.relative_to(data_dir / \"archived-query-urls\").with_name(path.name[:-len(\".jsonl.gz\")])\n",
    "    for path in data_dir.glob(\"archived-query-urls/*/*/*.jsonl.gz\")\n",
    "]\n",
    "shuffle(relative_paths)\n",
    "len(relative_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34798975-b940-4b4f-a88f-5a620e3cacd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from yaml import safe_load\n",
    "\n",
    "with Path(\"../data/selected-services.yaml\").open(\"r\") as file:\n",
    "    services_dict = safe_load(file)\n",
    "services_list = [(service[\"name\"], service) for service in services_dict]\n",
    "assert len({name for name, service in services_list}) == len(services_list)\n",
    "services = {\n",
    "    name: service\n",
    "    for name, service in services_list\n",
    "}\n",
    "len(services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1dfbfaf5-f625-4574-84f4-d6f8b0ac7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> Optional[str]:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    from cld3 import get_language\n",
    "    language_prediction = get_language(text)\n",
    "    if language_prediction is None:\n",
    "        return None\n",
    "    return language_prediction.language.split(\"-\")[0] if language_prediction.is_reliable else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea07bd99-f67b-45b0-8b78-717229f38c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from publicsuffixlist import PublicSuffixList\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "public_suffix_list = PublicSuffixList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8ce09c2-7698-417d-be56-86649ced717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid5, NAMESPACE_URL, UUID\n",
    "from tqdm.auto import tqdm\n",
    "from json import loads\n",
    "from gzip import GzipFile\n",
    "from typing import Iterator\n",
    "\n",
    "def relative_path_record_ids(relative_path: Path) -> Iterator[tuple]:\n",
    "    service = relative_path.parts[0]\n",
    "\n",
    "    jsonl_path = data_dir / \"archived-query-urls\" / relative_path.with_suffix(\".jsonl.gz\")\n",
    "    if not jsonl_path.exists():\n",
    "        return\n",
    "    with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "        for line in tqdm(gzip_file, desc=\"Read IDs from JSONL\"):\n",
    "            try:\n",
    "                record = loads(line)\n",
    "                record_id = uuid5(NAMESPACE_URL, f\"{record['timestamp']}:{record['url']}\")\n",
    "                yield service, relative_path, record_id\n",
    "            except:\n",
    "                print(f\"Could not index {line} at {relative_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bf4d44d-33cc-4224-8330-3b2a44bfa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def _relative_path_record_id_base(relative_path_record_id: tuple) -> tuple:\n",
    "    service: str\n",
    "    relative_path: Path\n",
    "    record_id: UUID\n",
    "    service, relative_path, record_id = relative_path_record_id\n",
    "    \n",
    "    archived_url: dict = None\n",
    "    jsonl_path = data_dir / \"archived-urls\" / relative_path.with_suffix(\".jsonl.gz\")\n",
    "    if jsonl_path.exists():\n",
    "        try:\n",
    "            with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "                for line in tqdm(gzip_file, desc=\"Read archived URLs from JSONL\"):\n",
    "                    offset = gzip_file.tell()\n",
    "                    try:\n",
    "                        record = loads(line)\n",
    "                    except:\n",
    "                        print(f\"Could not read JSON record {line} at {relative_path}.\")\n",
    "                        continue\n",
    "                    if record_id == uuid5(NAMESPACE_URL, f\"{record['timestamp']}:{record['url']}\"):\n",
    "                        archived_url = record\n",
    "                        break\n",
    "        except:\n",
    "            print(f\"Could not read JSONL file at {relative_path}.\")\n",
    "    \n",
    "    archived_query_url: dict = None\n",
    "    jsonl_path = data_dir / \"archived-query-urls\" / relative_path.with_suffix(\".jsonl.gz\")\n",
    "    if jsonl_path.exists():\n",
    "        try:\n",
    "            with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "                for line in tqdm(gzip_file, desc=\"Read archived query URLs from JSONL\"):\n",
    "                    offset = gzip_file.tell()\n",
    "                    try:\n",
    "                        record = loads(line)\n",
    "                    except:\n",
    "                        print(f\"Could not read JSON record {line} at {relative_path}.\")\n",
    "                        continue\n",
    "                    if record_id == uuid5(NAMESPACE_URL, f\"{record['timestamp']}:{record['url']}\"):\n",
    "                        archived_query_url = record\n",
    "                        break\n",
    "        except:\n",
    "            print(f\"Could not read JSONL file at {relative_path}.\")\n",
    "    \n",
    "    archived_raw_serp_location: tuple = None\n",
    "    warc_path = data_dir / \"archived-raw-serps\" / relative_path\n",
    "    if warc_path.exists():\n",
    "        for warc_child_path in warc_path.iterdir():\n",
    "            if warc_child_path.name.startswith(\".\"):\n",
    "                continue\n",
    "            try:\n",
    "                stream = FileStream(str(warc_child_path.absolute()))\n",
    "                records = ArchiveIterator(\n",
    "                    stream,\n",
    "                    record_types=WarcRecordType.response,\n",
    "                    parse_http=False,\n",
    "                )\n",
    "                for record in tqdm(records, desc=\"Read raw SERPs from WARC\"):\n",
    "                    record: WarcRecord\n",
    "                    offset = record.stream_pos\n",
    "                    record_url_header = record.headers[\"Archived-URL\"]\n",
    "                    try:\n",
    "                        record_url = loads(record_url_header)\n",
    "                    except JSONDecodeError:\n",
    "                        print(f\"Could not read WARC JSON header {record_url_header} at {relative_path}.\")\n",
    "                        continue\n",
    "                    if record_id == uuid5(NAMESPACE_URL, f\"{record_url['timestamp']}:{record_url['url']}\"):\n",
    "                        archived_raw_serp_location = (warc_child_path, offset)\n",
    "            except:\n",
    "                print(f\"Could not read WARC file at {warc_child_path}.\")\n",
    "    \n",
    "    archived_parsed_serp: dict = None\n",
    "    jsonl_path = data_dir / \"archived-parsed-serps\" / relative_path.with_suffix(\".jsonl.gz\")\n",
    "    if jsonl_path.exists():\n",
    "        try:\n",
    "            with GzipFile(jsonl_path, \"r\") as gzip_file:\n",
    "                for line in tqdm(gzip_file, desc=\"Read parsed SERPs from JSONL\"):\n",
    "                    offset = gzip_file.tell()\n",
    "                    try:\n",
    "                        record = loads(line)\n",
    "                    except:\n",
    "                        print(f\"Could not read JSON record {line} at {relative_path}.\")\n",
    "                        continue\n",
    "                    if record_id == uuid5(NAMESPACE_URL, f\"{record['timestamp']}:{record['url']}\"):\n",
    "                        archived_parsed_serp = record\n",
    "                        break\n",
    "        except:\n",
    "            print(f\"Could not read JSONL file at {relative_path}.\")\n",
    "            \n",
    "    return service, relative_path, record_id, archived_url, archived_query_url, archived_raw_serp_location, archived_parsed_serp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8abf17e5-3725-493e-b1c7-9cf4ed466dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_results(archived_url: dict, archived_parsed_serp: dict) -> Iterator[dict]:\n",
    "    if archived_parsed_serp is None:\n",
    "        return\n",
    "        \n",
    "    for snippet in archived_parsed_serp[\"results\"]:\n",
    "        url = snippet[\"url\"]\n",
    "        domain = urlparse(url).hostname\n",
    "        public_suffix = public_suffix_list.publicsuffix(domain)\n",
    "        timestamp = archived_url[\"timestamp\"]\n",
    "        wayback_timestamp = datetime.fromtimestamp(timestamp).strftime(\"%Y%m%d%H%M%S\")\n",
    "        wayback_url = f\"https://web.archive.org/web/{wayback_timestamp}/{url}\"\n",
    "        wayback_raw_url = f\"https://web.archive.org/web/{wayback_timestamp}id_/{url}\"\n",
    "        yield {\n",
    "            \"result_id\": str(uuid5(NAMESPACE_URL, f\"{snippet['rank']}:{snippet['timestamp']}:{snippet['url']}\")),\n",
    "            \"result_url\": url,\n",
    "            \"result_domain\": domain,\n",
    "            \"result_domain_public_suffix\": public_suffix,\n",
    "            \"result_wayback_url\": wayback_url,\n",
    "            \"result_wayback_raw_url\": wayback_raw_url,\n",
    "            \"result_snippet_rank\": snippet['rank'],\n",
    "            \"result_snippet_title\": snippet[\"title\"],\n",
    "            \"result_snippet_text\": snippet[\"snippet\"],\n",
    "            \"result_warc_relative_path\": None,\n",
    "            \"result_warc_byte_offset\": None,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67628eb6-6e40-45b8-b954-5b7b1e18b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def relative_path_record_id_queries(relative_path_record_id: tuple) -> Iterator[dict]:\n",
    "    service, relative_path, record_id, archived_url, archived_query_url, archived_raw_serp_location, archived_parsed_serp = _relative_path_record_id_base(relative_path_record_id)\n",
    "            \n",
    "    if archived_query_url is None:\n",
    "        yield \"empty\"\n",
    "        print(f\"Archived query URL not found for ID {record_id}.\")\n",
    "        return\n",
    "    \n",
    "    url = archived_url[\"url\"]\n",
    "    domain = urlparse(url).hostname\n",
    "    public_suffix = public_suffix_list.publicsuffix(domain)\n",
    "    timestamp = archived_url[\"timestamp\"]\n",
    "    wayback_timestamp = datetime.fromtimestamp(timestamp).strftime(\"%Y%m%d%H%M%S\")\n",
    "    wayback_url = f\"https://web.archive.org/web/{wayback_timestamp}/{url}\"\n",
    "    wayback_raw_url = f\"https://web.archive.org/web/{wayback_timestamp}id_/{url}\"\n",
    "    query = archived_query_url[\"query\"]\n",
    "    language = detect_language(query)\n",
    "    service_info = services[service]\n",
    "    \n",
    "    documents = list(_iter_results(archived_url, archived_parsed_serp))\n",
    "    \n",
    "    yield {\n",
    "        \"serp_id\": str(record_id),\n",
    "        \"serp_url\": url,\n",
    "        \"serp_domain\": domain,\n",
    "        \"serp_domain_public_suffix\": public_suffix,\n",
    "        \"serp_timestamp\": timestamp,\n",
    "        \"serp_wayback_url\": wayback_url,\n",
    "        \"serp_wayback_raw_url\": wayback_raw_url,\n",
    "        \"serp_page\": archived_query_url[\"page\"],\n",
    "        \"serp_offset\": archived_query_url[\"offset\"],\n",
    "        \"serp_query_text_url\": query,\n",
    "        \"serp_query_text_url_language\": language,\n",
    "        \"serp_query_text_html\": archived_parsed_serp[\"interpreted_query\"] if archived_parsed_serp is not None else None,\n",
    "        \"serp_warc_relative_path\": str(archived_raw_serp_location[0].relative_to(global_data_dir)) if archived_raw_serp_location is not None else None,\n",
    "        \"serp_warc_byte_offset\": archived_raw_serp_location[1] if archived_raw_serp_location is not None else None,\n",
    "        \"serp_results\": documents,\n",
    "        \"search_provider_name\": service,\n",
    "        \"search_provider_alexa_domain\": service_info[\"alexa_domain\"],\n",
    "        \"search_provider_alexa_domain_public_suffix\": services[service][\"public_suffix\"],\n",
    "        \"search_provider_alexa_rank\": service_info[\"alexa_rank\"],\n",
    "        \"search_provider_category\": service_info[\"category\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6122c1a3-b20c-4f88-945f-9279bec9b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(query: dict) -> Iterator[dict]:\n",
    "    for result in query[\"serp_results\"]:\n",
    "        yield {\n",
    "            \"result_id\": result[\"result_id\"],\n",
    "            \"result_url\": result[\"result_url\"],\n",
    "            \"result_domain\": result[\"result_domain\"],\n",
    "            \"result_domain_public_suffix\": result[\"result_domain_public_suffix\"],\n",
    "            \"result_wayback_url\": result[\"result_wayback_url\"],\n",
    "            \"result_wayback_raw_url\": result[\"result_wayback_raw_url\"],\n",
    "            \"result_snippet_rank\": result[\"result_snippet_rank\"],\n",
    "            \"result_snippet_title\": result[\"result_snippet_title\"],\n",
    "            \"result_snippet_text\": result[\"result_snippet_text\"],\n",
    "            \"result_warc_relative_path\": result[\"result_warc_relative_path\"],\n",
    "            \"result_warc_byte_offset\": result[\"result_warc_byte_offset\"],\n",
    "            \"serp_id\": query[\"serp_id\"],\n",
    "            \"serp_url\": query[\"serp_url\"],\n",
    "            \"serp_domain\": query[\"serp_domain\"],\n",
    "            \"serp_domain_public_suffix\": query[\"serp_domain_public_suffix\"],\n",
    "            \"serp_timestamp\": query[\"serp_timestamp\"],\n",
    "            \"serp_wayback_url\": query[\"serp_wayback_url\"],\n",
    "            \"serp_wayback_raw_url\": query[\"serp_wayback_raw_url\"],\n",
    "            \"serp_page\": query[\"serp_page\"],\n",
    "            \"serp_offset\": query[\"serp_offset\"],\n",
    "            \"serp_query_text_url\": query[\"serp_query_text_url\"],\n",
    "            \"serp_query_text_url_language\": query[\"serp_query_text_url_language\"],\n",
    "            \"serp_query_text_html\": query[\"serp_query_text_html\"],\n",
    "            \"serp_warc_relative_path\": query[\"serp_warc_relative_path\"],\n",
    "            \"serp_warc_byte_offset\": query[\"serp_warc_byte_offset\"],\n",
    "            \"search_provider_name\": query[\"search_provider_name\"],\n",
    "            \"search_provider_alexa_domain\": query[\"search_provider_alexa_domain\"],\n",
    "            \"search_provider_alexa_domain_public_suffix\": query[\"search_provider_alexa_domain_public_suffix\"],\n",
    "            \"search_provider_alexa_rank\": query[\"search_provider_alexa_rank\"],\n",
    "            \"search_provider_category\": query[\"search_provider_category\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4fc081a4-abb9-495b-b3ef-9611e456f524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted archive-query-log/serps\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r archive-query-log/serps/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cac2cb-eb93-46cd-86d0-d48d9028e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "\n",
    "sc.parallelize(relative_paths)\\\n",
    "    .repartition(1_000)\\\n",
    "    .flatMap(relative_path_record_ids)\\\n",
    "    .repartition(1_000)\\\n",
    "    .flatMap(relative_path_record_id_queries)\\\n",
    "    .map(dumps)\\\n",
    "    .repartition(100)\\\n",
    "    .saveAsTextFile(\n",
    "        \"archive-query-log/serps/\", \n",
    "        compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096b261-7888-4d33-85ce-4bf51169ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r archive-query-log/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2a687-655b-4b1a-9abb-53b73009c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "\n",
    "sc.parallelize(relative_paths)\\\n",
    "    .repartition(1_000)\\\n",
    "    .flatMap(relative_path_record_ids)\\\n",
    "    .repartition(1_000)\\\n",
    "    .flatMap(relative_path_record_id_queries)\\\n",
    "    .flatMap(query_documents)\\\n",
    "    .map(dumps)\\\n",
    "    .repartition(100)\\\n",
    "    .saveAsTextFile(\n",
    "        \"archive-query-log/results/\", \n",
    "        compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a33e0-02df-4676-b624-5c9477e10686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a9c19-973d-43d2-b2e1-3ed8c5add4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967cf74f-1d00-49a5-b5dd-2104111ea8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
