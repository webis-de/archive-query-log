{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from elasticsearch_dsl.connections import create_connection\n",
    "\n",
    "load_dotenv(override=True)\n",
    "create_connection(\n",
    "    hosts=\"https://elasticsearch.srv.webis.de:9200\",\n",
    "    http_auth=(\"\", \"\"),\n",
    "    timeout=60,\n",
    "    max_retries=5,\n",
    "    retry_on_status=(502, 503, 504),\n",
    "    retry_on_timeout=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from archive_query_log.orm import Provider\n",
    "\n",
    "\n",
    "providers: list[Provider] = list(Provider.search(index=\"aql_providers\").scan())\n",
    "providers_by_domain = {domain: p for p in providers for domain in p.domains}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb32274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from archive_query_log.orm import Archive\n",
    "\n",
    "archives: list[Archive] = list(Archive.search(index=\"aql_archives\").scan())\n",
    "archives_by_name = {a.name: a for a in archives}\n",
    "\n",
    "WAYBACK_MACHINE = archives_by_name[\"Internet Archive\"]\n",
    "WAYBACK_MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid5\n",
    "\n",
    "from pydantic import BaseModel, field_validator, HttpUrl\n",
    "\n",
    "from archive_query_log.namespaces import NAMESPACE_CAPTURE\n",
    "from archive_query_log.utils.time import CET, UTC\n",
    "from archive_query_log.orm import Serp, InnerArchive, InnerProvider, InnerCapture\n",
    "\n",
    "\n",
    "class _Serp(BaseModel):\n",
    "    timestamp: datetime\n",
    "    url: HttpUrl\n",
    "    query: str\n",
    "\n",
    "    @field_validator(\"timestamp\", mode=\"before\")\n",
    "    def parse_timestamp(cls, value) -> datetime:\n",
    "        timestamp = datetime.fromtimestamp(value, tz=UTC)\n",
    "        # Bug fix because the AQL-22 data is in CET, but the timestamps are\n",
    "        # not marked as such.\n",
    "        timestamp = timestamp.astimezone(CET)\n",
    "        timestamp = timestamp.replace(tzinfo=UTC)\n",
    "        return timestamp\n",
    "\n",
    "    def to_serp(\n",
    "        self,\n",
    "        last_modified: datetime,\n",
    "    ) -> Serp:\n",
    "        archive = WAYBACK_MACHINE\n",
    "        domain = self.url.host\n",
    "        if domain is None:\n",
    "            raise ValueError(f\"No provider found for domain {domain}\")\n",
    "        if domain.startswith(\"www.\"):\n",
    "            domain = domain[4:]\n",
    "        provider = providers_by_domain[domain]\n",
    "        path = self.url.path\n",
    "        if path is None:\n",
    "            raise ValueError(f\"No path found in URL {self.url}\")\n",
    "        path += \"?\"\n",
    "        url_path_prefixes = [\n",
    "            prefix for prefix in provider.url_path_prefixes if path.casefold().startswith(prefix)\n",
    "        ]\n",
    "        if len(url_path_prefixes) == 0:\n",
    "            raise ValueError(\n",
    "                f\"No matching URL path prefix found for URL {self.url} in provider {provider.id} ({path}; {provider.url_path_prefixes})\"\n",
    "            )\n",
    "        url_path_prefix = max(url_path_prefixes, key=len)\n",
    "        capture_id_components = (\n",
    "            archive.cdx_api_url.encoded_string(),\n",
    "            self.url.encoded_string(),\n",
    "            self.timestamp.astimezone(tz=UTC).strftime(\"%Y%m%d%H%M%S\"),\n",
    "        )\n",
    "        capture_id = uuid5(\n",
    "            NAMESPACE_CAPTURE,\n",
    "            \":\".join(capture_id_components),\n",
    "        )\n",
    "        return Serp(\n",
    "            id=capture_id,\n",
    "            last_modified=last_modified,\n",
    "            archive=InnerArchive(\n",
    "                id=archive.id,\n",
    "                cdx_api_url=archive.cdx_api_url,\n",
    "                memento_api_url=archive.memento_api_url,\n",
    "                priority=archive.priority,\n",
    "            ),\n",
    "            provider=InnerProvider(\n",
    "                id=provider.id,\n",
    "                domain=domain,\n",
    "                url_path_prefix=url_path_prefix,\n",
    "            ),\n",
    "            capture=InnerCapture(\n",
    "                id=capture_id,\n",
    "                url=self.url,\n",
    "                timestamp=self.timestamp,\n",
    "                status_code=200,\n",
    "                digest=\"\",\n",
    "                mimetype=\"text/html\",\n",
    "            ),\n",
    "            url_query=self.query,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "from warcio import ArchiveIterator\n",
    "from warcio.recordloader import ArcWarcRecord\n",
    "\n",
    "TEST_WARC_PATH = Path(\"../data/manual-annotations/archived-raw-serps/warcs\")\n",
    "\n",
    "\n",
    "def iter_test_serps() -> Iterator[tuple[str, Serp, ArcWarcRecord]]:\n",
    "    for path in TEST_WARC_PATH.glob(\"*.warc.gz\"):\n",
    "        name = path.name.split(\"-\")[0]\n",
    "        with open(path, \"rb\") as stream:\n",
    "            record: ArcWarcRecord\n",
    "            for record in ArchiveIterator(stream):\n",
    "                if record.rec_type == \"response\":\n",
    "                    archived_url = record.rec_headers[\"Archived-URL\"]\n",
    "                    del record.rec_headers[\"Archived-URL\"]\n",
    "                    legacy_serp = _Serp.model_validate_json(archived_url)\n",
    "                    serp = legacy_serp.to_serp(\n",
    "                        last_modified=datetime.fromisoformat(\n",
    "                            record.rec_headers[\"WARC-Date\"]\n",
    "                        ),\n",
    "                    )\n",
    "                    yield name, serp, record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed878c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_serps = list(iter_test_serps())\n",
    "test_serps.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS_PATH = Path(\"../data/tests/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "from warcio.recordloader import ArcWarcRecord\n",
    "from warcio.warcwriter import WARCWriter\n",
    "\n",
    "from archive_query_log.orm import WarcLocation\n",
    "\n",
    "for name, group in groupby(test_serps, key=lambda x: x[0]):\n",
    "    print(f\"Writing test case {name}\")\n",
    "    tests_path = TESTS_PATH / name\n",
    "\n",
    "    warc_path = tests_path.with_suffix(\".warc.gz\")\n",
    "    serp_path = tests_path.with_suffix(\".jsonl\")\n",
    "    with (\n",
    "        warc_path.open(\"wb\") as warc_file,\n",
    "        serp_path.open(\"wt\", encoding=\"utf-8\") as serp_file,\n",
    "    ):\n",
    "        writer = WARCWriter(warc_file, gzip=True)\n",
    "        # Write WARC info record.\n",
    "        warc_info_record: ArcWarcRecord = writer.create_warcinfo_record(\n",
    "            filename=warc_path.name, info={}\n",
    "        )\n",
    "        writer.write_record(warc_info_record)\n",
    "        for _, serp, record in group:\n",
    "            offset = warc_file.tell()\n",
    "            writer.write_record(record)\n",
    "            length = warc_file.tell() - offset\n",
    "            serp.warc_location = WarcLocation(\n",
    "                file=warc_path.name,\n",
    "                offset=offset,\n",
    "                length=length,\n",
    "            )\n",
    "            serp_file.write(serp.model_dump_json() + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
