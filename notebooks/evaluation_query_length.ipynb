{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 23:49:25 WARN Utils: Your hostname, reimer-thinkpad-x1-yoga resolves to a loopback address: 127.0.1.1; using 192.168.178.37 instead (on interface wlp0s20f3)\n",
      "23/02/09 23:49:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 23:49:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": "<SparkContext master=local[*] appName=web-archive-query-log-query-length>",
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.178.37:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>web-archive-query-log-query-length</code></dd>\n            </dl>\n        </div>\n        "
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=web-archive-query-log-query-length, master=local[*]) created by __init__ at /tmp/ipykernel_2426844/535384113.py:7 ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m conf \u001B[38;5;241m=\u001B[39m SparkConf()\n\u001B[1;32m      4\u001B[0m conf\u001B[38;5;241m.\u001B[39msetAll([\n\u001B[1;32m      5\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.instances\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m      6\u001B[0m ])\n\u001B[0;32m----> 7\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# TODO: For final evaluation, run on YARN cluster.\u001B[39;49;00m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# master=\"yarn\",\u001B[39;49;00m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mappName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweb-archive-query-log-query-length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m sc\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/web-archive-query-log-e4_6_sf5/lib/python3.10/site-packages/pyspark/context.py:195\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    192\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    193\u001B[0m     )\n\u001B[0;32m--> 195\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    198\u001B[0m         master,\n\u001B[1;32m    199\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    208\u001B[0m         udf_profiler_cls,\n\u001B[1;32m    209\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/web-archive-query-log-e4_6_sf5/lib/python3.10/site-packages/pyspark/context.py:430\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    427\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n\u001B[0;32m--> 430\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    431\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    432\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    433\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    434\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    435\u001B[0m             currentAppName,\n\u001B[1;32m    436\u001B[0m             currentMaster,\n\u001B[1;32m    437\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n\u001B[1;32m    438\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n\u001B[1;32m    439\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n\u001B[1;32m    440\u001B[0m         )\n\u001B[1;32m    441\u001B[0m     )\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    443\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=web-archive-query-log-query-length, master=local[*]) created by __init__ at /tmp/ipykernel_2426844/535384113.py:7 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.executor.instances\", 3)\n",
    "])\n",
    "sc = SparkContext(\n",
    "    # TODO: For final evaluation, run on YARN cluster.\n",
    "    # master=\"yarn\",\n",
    "    appName=\"web-archive-query-log-query-length\",\n",
    "    conf=conf,\n",
    ")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.178.37:4040\n"
     ]
    }
   ],
   "source": [
    "print(sc.uiWebUrl)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# TODO: For final evaluation, use the full corpus.\n",
    "# corpus_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/focused/corpus/\")\n",
    "corpus_dir = Path(\"/mnt/ceph/storage/data-in-progress/data-research/web-search/web-archive-query-log/focused/sample-corpus/\")\n",
    "queries_dir = corpus_dir / \"queries\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"figures\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def characteristics(query: dict) -> tuple:\n",
    "    return (\n",
    "        query[\"service\"],\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from json import loads\n",
    "\n",
    "def length_counts():\n",
    "    return sc.textFile(f\"file://{queries_dir}\")\\\n",
    "        .map(lambda line: loads(line))\\\n",
    "        .filter(lambda query: query[\"url_query\"] is not None) \\\n",
    "        .keyBy(lambda query: (len(query[\"url_query\"]), *characteristics(query)))\\\n",
    "        .countByKey()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame([\n",
    "    {\n",
    "        \"query_length\": length,\n",
    "        \"service\": service,\n",
    "        \"count\": count,\n",
    "    }\n",
    "    for (length, service), count in length_counts().items()\n",
    "])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from seaborn import histplot, kdeplot\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "fig: Figure\n",
    "fig, ax = subplots()\n",
    "# fig.tight_layout()\n",
    "df_vis = df[df[\"count\"] <= 100].iloc[:10]\n",
    "histplot(\n",
    "    data=df_vis,\n",
    "    x=\"query_length\",\n",
    "    weights=\"count\",\n",
    "    hue=\"service\",\n",
    "    bins=False,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"query length\")\n",
    "ax.set_ylabel(\"count\")\n",
    "# fig.savefig(figures_dir / \"query-length-histogram.pdf\")\n",
    "fig.savefig(figures_dir / \"query-length-histogram.png\")\n",
    "ax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
